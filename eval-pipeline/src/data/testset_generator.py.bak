"""
Hybrid Testset Generator for RAG Evaluation Pipeline

Combines RAGAS testset generation capabilities with existing auto-keyword
extraction methods from the domain-specific evaluation framework.
"""

import logging
import sys
import json
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

# Add parent directories to path to import existing code
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

try:
    from generate_dataset_configurable import ConfigurableDatasetGenerator
    CONFIGURABLE_GENERATOR_AVAILABLE = True
except ImportError:
    logging.warning("Could not import ConfigurableDatasetGenerator")
    CONFIGURABLE_GENERATOR_AVAILABLE = False

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    logging.warning("Pandas not available")
    PANDAS_AVAILABLE = False

try:
    # Try to import RAGAS components
    sys.path.append(str(Path(__file__).parent.parent.parent.parent / "ragas" / "src"))
    from ragas.testset import TestsetGenerator
    RAGAS_AVAILABLE = True
except ImportError:
    logging.warning("RAGAS TestsetGenerator not available")
    RAGAS_AVAILABLE = False

logger = logging.getLogger(__name__)

class HybridTestsetGenerator:
    """
    Hybrid testset generator that combines multiple approaches:
    1. Your existing configurable generator with auto-keyword extraction
    2. RAGAS TestsetGenerator (if available)
    3. Fallback simple generation
    """
    
    def __init__(self, config: Dict[str, Any], output_dir: Path):
        """
        Initialize hybrid testset generator.
        
        Args:
            config: Testset generation configuration
            output_dir: Output directory for testsets
        """
        self.config = config
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize generators based on availability
        self._initialize_generators()
    
    def _initialize_generators(self) -> None:
        """Initialize available testset generators."""
        self.generators = {}
        
        # Initialize configurable generator (your existing system)
        if CONFIGURABLE_GENERATOR_AVAILABLE:
            try:
                # Create temporary config file for configurable generator
                temp_config = self._create_configurable_generator_config()
                self.generators['configurable'] = {
                    'instance': ConfigurableDatasetGenerator(temp_config),
                    'available': True
                }
                logger.info("âœ… Configurable generator (auto-keyword) initialized")
            except Exception as e:
                logger.warning(f"Failed to initialize configurable generator: {e}")
                self.generators['configurable'] = {'available': False}
        else:
            self.generators['configurable'] = {'available': False}
        
        # Initialize RAGAS generator
        if RAGAS_AVAILABLE:
            try:
                # Will be initialized per document due to RAGAS requirements
                self.generators['ragas'] = {'available': True}
                logger.info("âœ… RAGAS generator available")
            except Exception as e:
                logger.warning(f"Failed to initialize RAGAS generator: {e}")
                self.generators['ragas'] = {'available': False}
        else:
            self.generators['ragas'] = {'available': False}
        
        # Fallback generator is always available
        self.generators['fallback'] = {'available': True}
        logger.info("âœ… Fallback generator initialized")
    
    def generate_testsets(self, processed_documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Generate testsets for all processed documents.
        
        Args:
            processed_documents: List of processed document dictionaries
            
        Returns:
            List of testset generation results
        """
        testset_results = []
        
        for doc in processed_documents:
            try:
                logger.info(f"ðŸŽ¯ Generating testset for: {doc['filename']}")
                
                # Generate testset for this document
                testset_result = self._generate_testset_for_document(doc)
                
                if testset_result:
                    testset_results.append(testset_result)
                    logger.info(f"âœ… Generated testset with {len(testset_result['qa_pairs'])} QA pairs")
                else:
                    logger.warning(f"Failed to generate testset for: {doc['filename']}")
                    
            except Exception as e:
                logger.error(f"Error generating testset for {doc['filename']}: {e}")
        
        return testset_results
    
    def _generate_testset_for_document(self, document: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Generate testset for a single document using best available method.
        
        Args:
            document: Processed document dictionary
            
        Returns:
            Testset generation result or None if failed
        """
        filename = document['filename']
        use_ragas = self.config.get('use_ragas_generator', True)
        
        # Try generators in order of preference
        generators_to_try = []
        
        if use_ragas and self.generators['ragas']['available']:
            generators_to_try.append('ragas')
        
        if self.generators['configurable']['available']:
            generators_to_try.append('configurable')
        
        generators_to_try.append('fallback')
        
        for generator_type in generators_to_try:
            try:
                logger.debug(f"Trying {generator_type} generator for {filename}")
                
                if generator_type == 'ragas':
                    result = self._generate_with_ragas(document)
                elif generator_type == 'configurable':
                    result = self._generate_with_configurable(document)
                else:  # fallback
                    result = self._generate_with_fallback(document)
                
                if result and len(result.get('qa_pairs', [])) > 0:
                    result['generator_used'] = generator_type
                    return result
                    
            except Exception as e:
                logger.warning(f"{generator_type} generator failed for {filename}: {e}")
                continue
        
        logger.error(f"All generators failed for: {filename}")
        return None
    
    def _generate_with_ragas(self, document: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Generate testset using RAGAS TestsetGenerator.
        
        Args:
            document: Processed document dictionary
            
        Returns:
            Testset generation result
        """
        if not RAGAS_AVAILABLE:
            raise Exception("RAGAS not available")
        
        # This is a simplified implementation
        # In reality, RAGAS requires more setup (LLM, embeddings, etc.)
        logger.info(f"Using RAGAS generator for {document['filename']}")
        
        # For now, return a placeholder
        # TODO: Implement full RAGAS integration
        raise Exception("RAGAS integration not yet fully implemented")
    
    def _generate_with_configurable(self, document: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Generate testset using your existing configurable generator with auto-keywords.
        
        Args:
            document: Processed document dictionary
            
        Returns:
            Testset generation result
        """
        if not self.generators['configurable']['available']:
            raise Exception("Configurable generator not available")
        
        logger.info(f"Using configurable generator (auto-keywords) for {document['filename']}")
        
        try:
            # Use your existing generator
            generator = self.generators['configurable']['instance']
            
            # This would need to be adapted to work with individual documents
            # For now, create a simple implementation
            qa_pairs = self._extract_qa_from_content(document['content'], method='configurable')
            
            # Generate output file
            output_file = self._save_testset(document, qa_pairs, 'configurable')
            
            return {
                'source_document': document['source_file'],
                'qa_pairs': qa_pairs,
                'output_file': output_file,
                'method': 'configurable_auto_keywords'
            }
            
        except Exception as e:
            logger.error(f"Configurable generator failed: {e}")
            raise
    
    def _generate_with_fallback(self, document: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Generate testset using simple fallback method.
        
        Args:
            document: Processed document dictionary
            
        Returns:
            Testset generation result
        """
        logger.info(f"Using fallback generator for {document['filename']}")
        
        # Simple extraction based on content structure
        qa_pairs = self._extract_qa_from_content(document['content'], method='fallback')
        
        if not qa_pairs:
            return None
        
        # Generate output file
        output_file = self._save_testset(document, qa_pairs, 'fallback')
        
        return {
            'source_document': document['source_file'],
            'qa_pairs': qa_pairs,
            'output_file': output_file,
            'method': 'fallback'
        }
    
    def _extract_qa_from_content(self, content: str, method: str = 'fallback') -> List[Dict[str, Any]]:
        """
        Extract QA pairs from document content.
        
        Args:
            content: Document content
            method: Extraction method to use
            
        Returns:
            List of QA pair dictionaries
        """
        qa_pairs = []
        samples_to_generate = min(
            self.config.get('samples_per_document', 20),
            len(content.split('\n')) // 5  # Rough heuristic
        )
        
        # Split content into chunks
        sentences = self._split_into_sentences(content)
        
        if len(sentences) < 3:
            logger.warning("Document too short for QA generation")
            return []
        
        # Generate QA pairs based on content
        for i in range(min(samples_to_generate, len(sentences) - 2)):
            try:
                qa_pair = self._create_qa_pair(sentences, i, method)
                if qa_pair:
                    qa_pairs.append(qa_pair)
            except Exception as e:
                logger.debug(f"Failed to create QA pair {i}: {e}")
        
        return qa_pairs
    
    def _split_into_sentences(self, content: str) -> List[str]:
        """Split content into sentences."""
        import re
        
        # Simple sentence splitting
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        
        return sentences
    
    def _create_qa_pair(self, sentences: List[str], index: int, method: str) -> Optional[Dict[str, Any]]:
        """
        Create a QA pair from content.
        
        Args:
            sentences: List of sentences
            index: Current sentence index
            method: Generation method
            
        Returns:
            QA pair dictionary
        """
        # Use current and next sentences as context
        context_sentences = sentences[index:index+3]
        context = ' '.join(context_sentences)
        
        if len(context.split()) < 10:
            return None
        
        # Extract key terms for keywords (simplified)
        keywords = self._extract_keywords_simple(context)
        
        # Generate question and answer (simplified)
        question = self._generate_question_simple(context)
        answer = context_sentences[0]  # First sentence as answer
        
        return {
            'question': question,
            'answer': answer,
            'contexts': [context],
            'ground_truth': answer,
            'keywords': keywords,
            'source_method': method
        }
    
    def _extract_keywords_simple(self, text: str) -> List[str]:
        """
        Simple keyword extraction.
        
        Args:
            text: Input text
            
        Returns:
            List of keywords
        """
        # Simple approach: extract capitalized words and important terms
        import re
        
        words = text.split()
        keywords = []
        
        # Extract capitalized words (likely important terms)
        for word in words:
            if word[0].isupper() and len(word) > 3:
                clean_word = re.sub(r'[^\w\s]', '', word)
                if clean_word:
                    keywords.append(clean_word.lower())
        
        # Limit to reasonable number
        return list(set(keywords))[:5]
    
    def _generate_question_simple(self, context: str) -> str:
        """
        Generate a simple question from context.
        
        Args:
            context: Context text
            
        Returns:
            Generated question
        """
        # Very simple question generation
        words = context.split()
        
        if len(words) < 5:
            return "What is described in the document?"
        
        # Extract first noun or important term
        important_word = None
        for word in words[:10]:
            if len(word) > 4 and word[0].isupper():
                important_word = word.rstrip('.,!?')
                break
        
        if important_word:
            return f"What is {important_word}?"
        else:
            return "What is described in this context?"
    
    def _save_testset(self, document: Dict[str, Any], qa_pairs: List[Dict[str, Any]], method: str) -> str:
        """
        Save testset to file.
        
        Args:
            document: Source document
            qa_pairs: Generated QA pairs
            method: Generation method used
            
        Returns:
            Path to saved file
        """
        # Create filename
        base_name = Path(document['filename']).stem
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"testset_{base_name}_{method}_{timestamp}.xlsx"
        
        output_file = self.output_dirs / filename
        
        if PANDAS_AVAILABLE:
            # Save as Excel file
            df = pd.DataFrame(qa_pairs)
            df.to_excel(output_file, index=False)
        else:
            # Save as JSON fallback
            output_file = output_file.with_suffix('.json')
            with open(output_file, 'w') as f:
                json.dump({
                    'source_document': document['source_file'],
                    'generation_method': method,
                    'timestamp': timestamp,
                    'qa_pairs': qa_pairs
                }, f, indent=2)
        
        logger.info(f"Testset saved to: {output_file}")
        return str(output_file)
    
    def _create_configurable_generator_config(self) -> str:
        """
        Create configuration file for the configurable generator.
        
        Returns:
            Path to temporary config file
        """
        # Create a temporary config based on pipeline config
        temp_config = {
            'mode': 'local',  # Use local mode for auto-keyword extraction
            'dataset': {
                'num_samples': self.config.get('samples_per_document', 20),
                'output_file': 'temp_testset.xlsx'
            },
            'local': {
                'sentence_model': 'all-MiniLM-L6-v2',
                'keybert': {'enabled': True},
                'yake': {'enabled': True}
            }
        }
        
        # Save temporary config
        temp_config_file = self.output_dir / 'temp_config.yaml'
        import yaml
        with open(temp_config_file, 'w') as f:
            yaml.dump(temp_config, f)
        
        return str(temp_config_file)
