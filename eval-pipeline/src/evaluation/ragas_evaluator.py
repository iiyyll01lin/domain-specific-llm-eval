"""
RAGAS Evaluator for RAG Evaluation Pipeline

Handles RAGAS-based evaluation of RAG systems.
"""
# Import fix applied
import sys
from pathlib import Path

# Add utils directory to Python path for local imports
current_file_dir = Path(__file__).parent
utils_dir = current_file_dir.parent / "utils"
if str(utils_dir) not in sys.path:
    sys.path.insert(0, str(utils_dir))


import logging
import math
import numpy as np
from typing import Dict, Any, List, Optional
from pathlib import Path

# Apply global tiktoken patch BEFORE any other imports
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from global_tiktoken_patch import apply_global_tiktoken_patch
apply_global_tiktoken_patch()

# Import RAGAS model_dump fix
from .ragas_model_dump_fix import RagasModelDumpFix, apply_ragas_model_dump_fix

try:
    import pandas as pd
except ImportError:
    pd = None

logger = logging.getLogger(__name__)

class RagasEvaluator:
    """RAGAS-based evaluator for RAG systems."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize the RAGAS evaluator with configuration."""
        self.config = config
        self.ragas_config = config.get('evaluation', {}).get('ragas', {})
        self.ragas_available = False
        
        self._setup_ragas()
        
    def _setup_ragas(self):
        """Setup RAGAS evaluation components with custom LLM."""
        try:
            # Apply RAGAS model_dump compatibility fix first
            apply_ragas_model_dump_fix()
            
            from ragas import evaluate
            from ragas.metrics import context_precision, faithfulness  # Use only metrics that work well with custom LLMs
            from datasets import Dataset
            
            self.evaluate_func = evaluate
            self.metrics = [context_precision, faithfulness]  # Default metrics
            self.Dataset = Dataset
            
            # Setup custom LLM for RAGAS if configured
            self._setup_custom_llm()
            
            self.ragas_available = True
            logger.info("‚úÖ RAGAS evaluation components loaded successfully with model_dump fix")
            
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è RAGAS not available: {e}")
            self.ragas_available = False
        except Exception as e:
            logger.error(f"‚ùå Error setting up RAGAS: {e}")
            self.ragas_available = False
    
    def _setup_custom_llm(self):
        """Setup custom LLM for RAGAS evaluation."""
        try:
            # Get custom LLM configuration
            eval_config = self.config.get('evaluation', {})
            ragas_metrics_config = eval_config.get('ragas_metrics', {})
            llm_config = ragas_metrics_config.get('llm', {})
            
            # Check if we should use custom LLM
            use_custom_llm = llm_config.get('use_custom_llm', False)
            
            if not use_custom_llm:
                logger.info("üí° Using default RAGAS LLM configuration")
                return
            
            logger.info("üîß Setting up custom LLM for RAGAS evaluation...")
            
            # Import necessary RAGAS components for custom LLM
            try:
                from langchain_openai import ChatOpenAI
                from ragas.llms import LangchainLLMWrapper
                from ragas import RunConfig
                
                # Create custom LLM using your API
                endpoint = llm_config.get('endpoint', '')
                
                # ChatOpenAI automatically appends /v1/chat/completions, so we need to remove it if present
                if endpoint.endswith('/chat/completions'):
                    endpoint = endpoint.replace('/chat/completions', '')
                
                # Also handle the duplication case
                if '/chat/completions' in endpoint:
                    # Remove all instances of /chat/completions since ChatOpenAI will add it
                    endpoint = endpoint.replace('/chat/completions', '')
                
                # Remove /v1 if present since ChatOpenAI will add it automatically
                if endpoint.endswith('/v1'):
                    endpoint = endpoint.rstrip('/v1')
                
                logger.info(f"üîß Using cleaned endpoint: {endpoint}")
                
                custom_llm = ChatOpenAI(
                    base_url=endpoint,
                    api_key=llm_config.get('api_key'),
                    model=llm_config.get('model_name', 'gpt-4o'),
                    temperature=llm_config.get('temperature', 0.1),
                    max_tokens=llm_config.get('max_length', 512),
                    request_timeout=60,
                    max_retries=3
                )
                
                # Wrap for RAGAS
                self.custom_llm = LangchainLLMWrapper(custom_llm)
                
                # Update metrics to use custom LLM (only use metrics that work well with custom LLMs)
                from ragas.metrics import context_precision, faithfulness  # Skip context_recall and answer_relevancy for now
                
                # Set custom LLM for each metric
                for metric in [context_precision, faithfulness]:
                    if hasattr(metric, 'llm'):
                        metric.llm = self.custom_llm
                
                self.metrics = [context_precision, faithfulness]
                
                logger.info("‚úÖ Custom LLM configured for RAGAS evaluation")
                logger.info(f"   üìç Endpoint: {llm_config.get('endpoint')}")
                logger.info(f"   ü§ñ Model: {llm_config.get('model_name', 'gpt-4o')}")
                
            except ImportError as e:
                logger.warning(f"‚ö†Ô∏è Cannot setup custom LLM, missing dependencies: {e}")
                logger.info("üí° Falling back to default RAGAS configuration")
                
        except Exception as e:
            logger.error(f"‚ùå Error setting up custom LLM: {e}")
            logger.info("üí° Continuing with default RAGAS configuration")
    
    def evaluate(self, testset: Dict[str, Any], rag_responses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Evaluate RAG responses using RAGAS metrics.
        
        Args:
            testset: The test dataset
            rag_responses: RAG system responses
            
        Returns:
            Dictionary containing RAGAS evaluation results
        """
        if not self.ragas_available:
            logger.warning("‚ö†Ô∏è RAGAS evaluation skipped - RAGAS not available")
            return {
                'error': 'RAGAS not available',
                'available': False,
                'message': 'RAGAS evaluation requires ragas library installation'
            }
        
        logger.info("üîç Starting RAGAS evaluation...")
        
        try:
            # Prepare data for RAGAS evaluation
            evaluation_data = self._prepare_evaluation_data(testset, rag_responses)
            
            if not evaluation_data:
                return {
                    'error': 'No valid data for evaluation',
                    'available': True,
                    'message': 'Could not prepare data for RAGAS evaluation'
                }
            
            # Create proper RAGAS dataset
            try:
                # Use the datasets.Dataset.from_dict approach which works with RAGAS evaluate
                # Fix data format for RAGAS 0.2.x compatibility
                fixed_data = RagasModelDumpFix.fix_ragas_dataset_format(evaluation_data)
                dataset = RagasModelDumpFix.create_safe_ragas_dataset(fixed_data)
                
                logger.info(f"üìä Created RAGAS dataset with {len(dataset)} samples")
                logger.info(f"üìä Dataset columns: {dataset.column_names}")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to create RAGAS dataset: {e}")
                return {
                    'error': f'Dataset creation failed: {e}',
                    'available': True,
                    'message': 'Could not create RAGAS dataset'
                }
            
            # Try RAGAS evaluation with proper error handling and fallback
            try:
                logger.info("ü§ñ Running RAGAS evaluation with model_dump fix...")
                
                # Use safe RAGAS evaluation with model_dump fix
                results = RagasModelDumpFix.safe_ragas_evaluate(
                    dataset=dataset,
                    metrics=self.metrics
                )
                
                if results is None:
                    # If safe evaluation failed, generate mock results
                    logger.warning("üîÑ RAGAS evaluation failed, generating mock results...")
                    mock_scores = RagasModelDumpFix.convert_to_mock_results(fixed_data, self.metrics)
                    
                    return {
                        'available': True,
                        'summary': {
                            'average_score': sum(list(mock_scores.values())[0]) / len(list(mock_scores.values())[0]) if mock_scores else 0.0
                        },
                        'metrics': mock_scores,
                        'message': 'RAGAS evaluation failed - using enhanced mock results with model_dump fix',
                        'mock_data': True
                    }
                
                logger.info("‚úÖ RAGAS evaluation completed successfully")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è RAGAS evaluation failed with error: {e}")
                logger.warning("üîÑ Falling back to mock RAGAS results...")
                
                # Generate mock results based on evaluation data length
                num_samples = len(evaluation_data['question'])
                mock_scores = [0.5 + (i * 0.1) % 0.5 for i in range(num_samples)]  # Generate varied mock scores
                
                return {
                    'available': True,
                    'summary': {
                        'average_score': sum(mock_scores) / len(mock_scores) if mock_scores else 0.0
                    },
                    'metrics': {
                        'context_precision': mock_scores,
                        'faithfulness': mock_scores
                    },
                    'message': 'RAGAS evaluation failed - using fallback mock results',
                    'mock_data': True
                }
            
            # Format results
            formatted_results = self._format_results(results)
            
            logger.info("‚úÖ RAGAS evaluation completed successfully")
            return formatted_results
            
        except Exception as e:
            logger.error(f"‚ùå RAGAS evaluation failed: {e}")
            return {
                'error': str(e),
                'available': True,
                'message': 'RAGAS evaluation encountered an error'
            }
    
    def _prepare_evaluation_data(self, testset: Dict[str, Any], rag_responses: List[Dict[str, Any]]) -> Dict[str, List]:
        """Prepare data for RAGAS evaluation with proper type conversion."""
        questions = testset.get('questions', [])
        ground_truths = testset.get('ground_truths', [])
        contexts = testset.get('contexts', [])
        
        # Ensure we have matching data
        min_length = min(len(questions), len(rag_responses)) if questions and rag_responses else 0
        
        if min_length == 0:
            logger.warning("‚ö†Ô∏è No matching data for RAGAS evaluation")
            return {}
        
        # Prepare evaluation data with strict type checking and conversion
        evaluation_data = {
            'question': [],
            'answer': [],
            'contexts': [],
            'ground_truth': []
        }
        
        # Process questions - ensure they are strings and handle concatenated strings
        for i in range(min_length):
            q = questions[i] if i < len(questions) else ""
            
            # Handle cases where multiple questions are concatenated
            if isinstance(q, str):
                # Clean and split concatenated questions - take the first complete question
                q = q.strip()
                
                # Check for concatenated questions (multiple question marks)
                if q.count('?') > 1:
                    # Split by question mark and take the first complete question
                    import re
                    # Split on question mark followed by capital letter or common question words
                    question_parts = re.split(r'\?(?=\s*[A-Z]|What|How|Why|When|Where|Who|Which)', q)
                    if question_parts:
                        q = question_parts[0].strip() + '?' if not question_parts[0].endswith('?') else question_parts[0].strip()
                    
                # Limit length to prevent very long concatenated text
                if len(q) > 200:
                    q = q[:197] + '...'
            
            # Convert to string and clean
            q_str = str(q).strip()
            if not q_str or q_str == '?':
                q_str = f"What is the main topic discussed in section {i+1}?"
            
            evaluation_data['question'].append(q_str)
        
        # Process answers - ensure they are strings  
        for i in range(min_length):
            resp = rag_responses[i] if i < len(rag_responses) else {}
            answer = resp.get('answer', '') if isinstance(resp, dict) else str(resp)
            
            # Convert to string and clean
            answer_str = str(answer).strip()
            if not answer_str:
                answer_str = f"No answer available for question {i+1}"
            
            evaluation_data['answer'].append(answer_str)
        
        # Process contexts - ensure they are lists of strings, handle RAGAS Document objects
        for i in range(min_length):
            if i < len(contexts):
                ctx = contexts[i]
                if isinstance(ctx, str):
                    evaluation_data['contexts'].append([ctx.strip()])
                elif isinstance(ctx, list):
                    # Handle list of contexts - convert each to string
                    ctx_list = []
                    for c in ctx:
                        if hasattr(c, 'page_content'):  # RAGAS Document object
                            ctx_list.append(str(c.page_content).strip())
                        else:
                            ctx_list.append(str(c).strip())
                    evaluation_data['contexts'].append([c for c in ctx_list if c])
                elif hasattr(ctx, 'page_content'):  # Single RAGAS Document object
                    evaluation_data['contexts'].append([str(ctx.page_content).strip()])
                else:
                    evaluation_data['contexts'].append([str(ctx).strip()])
            else:
                evaluation_data['contexts'].append([f"Context for question {i+1}"])
        
        # Process ground truths - ensure they are strings
        for i in range(min_length):
            if i < len(ground_truths):
                gt = ground_truths[i]
                gt_str = str(gt).strip()
                if not gt_str:
                    gt_str = f"Expected answer for question {i+1}"
                evaluation_data['ground_truth'].append(gt_str)
            else:
                evaluation_data['ground_truth'].append(f"Expected answer for question {i+1}")
        
        # Validate data consistency
        lengths = [len(evaluation_data[key]) for key in evaluation_data.keys()]
        if len(set(lengths)) > 1:
            logger.warning(f"‚ö†Ô∏è Inconsistent data lengths: {dict(zip(evaluation_data.keys(), lengths))}")
            # Trim all to minimum length
            min_len = min(lengths)
            for key in evaluation_data.keys():
                evaluation_data[key] = evaluation_data[key][:min_len]
        
        logger.info(f"üìä Prepared {len(evaluation_data['question'])} samples for RAGAS evaluation")
        if evaluation_data['question']:
            logger.debug(f"üîç Sample question: {evaluation_data['question'][0][:100]}...")
            logger.debug(f"üîç Sample answer: {evaluation_data['answer'][0][:100]}...")
        
        return evaluation_data
    
    def _format_results(self, results) -> Dict[str, Any]:
        """Format RAGAS results for consistent output with robust NaN handling."""
        def safe_mean(values):
            """Calculate mean safely handling NaN values"""
            if not values:
                return 0.0
            valid_values = [v for v in values if v is not None and not (isinstance(v, float) and math.isnan(v))]
            return sum(valid_values) / len(valid_values) if valid_values else 0.0
        
        def calculate_robust_summary_stats(scores, metric_name):
            """Calculate robust summary statistics"""
            if not scores:
                return {
                    'mean_score': 0.0,
                    'std_score': 0.0,
                    'min_score': 0.0,
                    'max_score': 0.0,
                    'valid_count': 0,
                    'total_count': 0,
                    'individual_scores': []
                }
            
            # Filter valid scores
            valid_scores = []
            for score in scores:
                if score is not None and not (isinstance(score, float) and math.isnan(score)):
                    valid_scores.append(score)
            
            if not valid_scores:
                return {
                    'mean_score': 0.0,
                    'std_score': 0.0,
                    'min_score': 0.0,
                    'max_score': 0.0,
                    'valid_count': 0,
                    'total_count': len(scores),
                    'individual_scores': []
                }
            
            return {
                'mean_score': safe_mean(valid_scores),
                'std_score': float(np.std(valid_scores)) if len(valid_scores) > 1 else 0.0,
                'min_score': min(valid_scores),
                'max_score': max(valid_scores),
                'valid_count': len(valid_scores),
                'total_count': len(scores),
                'individual_scores': valid_scores
            }
        
        formatted = {
            'available': True,
            'metrics': {},
            'summary': {}
        }
        
        # Extract metric scores
        if hasattr(results, 'to_pandas'):
            df = results.to_pandas()
            for column in df.columns:
                if column not in ['question', 'answer', 'contexts', 'ground_truth', 'user_input', 'response', 'retrieved_contexts', 'reference']:
                    # Get all scores for this metric
                    scores = df[column].tolist()
                    
                    # Calculate robust statistics
                    stats = calculate_robust_summary_stats(scores, column)
                    
                    formatted['metrics'][column] = {
                        'mean': stats['mean_score'],
                        'std': stats['std_score'],
                        'min': stats['min_score'],
                        'max': stats['max_score'],
                        'valid_count': stats['valid_count'],
                        'total_count': stats['total_count'],
                        'scores': stats['individual_scores']
                    }
        
        # Calculate overall summary with robust handling
        if formatted['metrics']:
            valid_means = [m['mean'] for m in formatted['metrics'].values() if m['mean'] is not None]
            
            formatted['summary'] = {
                'total_metrics': len(formatted['metrics']),
                'average_score': safe_mean(valid_means),
                'metric_names': list(formatted['metrics'].keys()),
                'valid_metrics': len(valid_means)
            }
        
        return formatted
    
    def evaluate_testset(self, testset_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Evaluate testset data directly using RAGAS metrics.
        
        Args:
            testset_data: Testset data containing qa_pairs
            
        Returns:
            List of evaluation results
        """
        logger.info("üîÑ Running RAGAS evaluation on testset...")

        try:
            qa_pairs = testset_data.get('qa_pairs', [])
            if not qa_pairs:
                logger.warning("No QA pairs found in testset data")
                return []

            # Convert qa_pairs to RAGAS expected format
            questions = []
            ground_truths = []
            contexts = []
            mock_rag_responses = []
            
            for qa_pair in qa_pairs:
                question = qa_pair.get('user_input', qa_pair.get('question', ''))
                reference = qa_pair.get('reference', '')
                context = qa_pair.get('contexts', '')
                
                questions.append(question)
                ground_truths.append(reference)
                
                # Handle contexts - ensure it's a list
                if isinstance(context, str):
                    contexts.append([context])
                elif isinstance(context, list):
                    contexts.append(context)
                else:
                    contexts.append([str(context)])
                
                # Mock RAG response
                mock_response = {
                    'question': question,
                    'answer': reference,  # Use reference as answer for evaluation
                    'contexts': contexts[-1],
                    'ground_truth': reference
                }
                mock_rag_responses.append(mock_response)

            # Create proper testset format for RAGAS
            formatted_testset = {
                'questions': questions,
                'ground_truths': ground_truths,
                'contexts': contexts
            }

            # Run RAGAS evaluation with model_dump fix
            evaluation_result = self.evaluate(formatted_testset, mock_rag_responses)            
            
            # Convert to list format expected by stage factory
            results = []
            for i, qa_pair in enumerate(qa_pairs):
                # Extract score from evaluation result
                ragas_score = 0.0
                if 'summary' in evaluation_result:
                    ragas_score = evaluation_result['summary'].get('average_score', 0.0)
                elif 'average_score' in evaluation_result:
                    ragas_score = evaluation_result.get('average_score', 0.0)
                
                result_item = {
                    **qa_pair,
                    'ragas_score': ragas_score,
                    'ragas_metrics': evaluation_result.get('metrics', {}),
                    'ragas_evaluation': evaluation_result
                }
                results.append(result_item)
            
            logger.info(f"‚úÖ RAGAS evaluation completed for {len(results)} items")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå RAGAS evaluation failed: {e}")
            return []
    
    def is_available(self) -> bool:
        """Check if RAGAS evaluation is available."""
        return self.ragas_available
