# Configuration for Synthetic Dataset Generator
# Choose between 'local' and 'ragas' modes

# MODE OPTIONS:
# - 'local': Use purely local models (sentence-transformers, KeyBERT) - No API keys required
# - 'ragas': Use RAGAS library with local or OpenAI models
# - 'simple': Use simple simulation without external models
mode: 'local'

# LOCAL MODE SETTINGS
local:
  # Sentence transformer model for similarity calculations
  sentence_model: 'all-MiniLM-L6-v2'  # Fast and lightweight
  # sentence_model: 'all-mpnet-base-v2'  # Better quality but slower
  
  # Keyword extraction settings
  keybert:
    enabled: true
    keyphrase_ngram_range: [1, 2]
    stop_words: 'english'
    max_keywords: 5
  
  yake:
    enabled: true
    language: 'en'
    n: 3
    dedupLim: 0.7
    top: 10
    max_keywords: 5
  
  # Metric calculation thresholds
  thresholds:
    relevance_threshold: 0.3
    similarity_min: 0.1
    similarity_max: 1.0

# RAGAS MODE SETTINGS
ragas:
  # Use local models instead of OpenAI
  use_local_llm: true
  
  # Local LLM settings (when use_local_llm: true)
  local_llm:
    model_name: 'microsoft/DialoGPT-small'  # Small, fast model
    # model_name: 'microsoft/DialoGPT-medium'  # Better quality
    max_length: 512
    device: 'auto'  # 'auto', 'cpu', or 'cuda'
  
  # OpenAI settings (when use_local_llm: false)
  openai:
    model: 'gpt-3.5-turbo'
    api_key: null  # Set your API key here or use environment variable

# CUSTOM DATA SETTINGS (Option 3: Use Your Own Raw Data)
custom_data:
  enabled: false  # Set to true to use your own documents
  
  # Data source configuration
  source_type: 'file'  # 'file', 'directory', 'url'
  
  # File/Directory paths for your raw documents
  data_sources:
    # PDF files
    pdf_files:
      - 'path/to/your/document1.pdf'
      - 'path/to/your/document2.pdf'
    
    # Text files
    text_files:
      - 'path/to/your/document1.txt'
      - 'path/to/your/document2.txt'
    
    # Word documents
    word_files:
      - 'path/to/your/document1.docx'
      - 'path/to/your/document2.docx'
    
    # Directories containing documents
    directories:
      - 'path/to/your/documents/folder'
    
    # CSV/Excel files with structured data
    structured_files:
      - file: 'path/to/your/data.csv'
        document_column: 'content'
        topic_column: 'category'
      - file: 'path/to/your/data.xlsx'
        document_column: 'text'
        topic_column: 'domain'
  
  # Document processing settings
  processing:
    # Text chunking for large documents
    chunk_size: 1000  # Characters per chunk
    chunk_overlap: 200  # Overlap between chunks
    min_chunk_size: 100  # Minimum chunk size to keep
    
    # Text cleaning
    remove_headers_footers: true
    remove_page_numbers: true
    remove_extra_whitespace: true
    
    # Language detection and filtering
    filter_by_language: true
    target_language: 'en'
    min_confidence: 0.8
  
  # Topic extraction from your documents
  topic_extraction:
    enabled: true
    method: 'keybert'  # 'keybert', 'yake', 'tfidf', 'manual'
    max_topics_per_document: 3
    
    # Manual topics (if method is 'manual')
    manual_topics:
      - 'your_topic_1'
      - 'your_topic_2'
      - 'your_topic_3'
  
  # Question generation templates for your domain
  question_templates:
    - "What is {} according to the document?"
    - "How does the document explain {}?"
    - "What are the key aspects of {} mentioned?"
    - "Can you summarize the information about {}?"
    - "What does the document say about {}?"
    - "How is {} defined in this context?"
    - "What are the characteristics of {}?"
    - "What information is provided about {}?"

# DATASET GENERATION SETTINGS
dataset:
  num_samples: 10
  output_file: 'SystemQAListallQuestion_eval_step4_final_report 1.xlsx'
  
  # Domains to include (when using default data)
  domains:
    medical: true
    technology: true
  
  # Quality distribution for synthetic answers
  answer_quality_distribution:
    high: 0.4
    medium: 0.4  
    low: 0.2

# FALLBACK SETTINGS
fallback:
  # Use simulation when models fail
  use_simulation: true
  
  # Random score ranges for simulation
  score_ranges:
    context_precision: [0.6, 0.95]
    context_recall: [0.6, 0.95] 
    faithfulness: [0.6, 0.95]
    answer_relevancy: [0.6, 0.95]
    kw_metric: [0.5, 0.9]

# LOGGING
logging:
  level: 'INFO'  # DEBUG, INFO, WARNING, ERROR
  show_progress: true